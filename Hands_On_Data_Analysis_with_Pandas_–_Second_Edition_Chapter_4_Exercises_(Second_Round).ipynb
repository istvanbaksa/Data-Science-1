{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hands-On Data Analysis with Pandas – Second Edition Chapter 4 Exercises (Second Round).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "177Ol-Vr3AU0KQXtK5rjDz2LWfRGsH1tX",
      "authorship_tag": "ABX9TyOYMRNqHg3FRjXFuYwY6aFL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/istvanbaksa/Data-Science-1/blob/main/Hands_On_Data_Analysis_with_Pandas_%E2%80%93_Second_Edition_Chapter_4_Exercises_(Second_Round).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hands-On Data Analysis with Pandas – Second Edition Chapter 4 Exercises (Second Round)"
      ],
      "metadata": {
        "id": "1z_91P4Btvx8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6ROiWFUtjDl"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "1. With the earthquakes.csv file, select all the earthquakes in Japan with a\n",
        "magnitude of 4.9 or greater using the mb magnitude type.\n",
        "'''\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "pd.set_option('display.max_rows', 5000)\n",
        "pd.set_option('display.max_columns', 5000)\n",
        "\n",
        "earthquakes = pd.read_csv('https://raw.githubusercontent.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/master/ch_02/data/earthquakes.csv')\n",
        "\n",
        "earthquakes_jap = earthquakes[(earthquakes.place.str.contains('Japan')) & (earthquakes.magType == 'mb') & (earthquakes.mag >= 4.9)]\n",
        "display(earthquakes_jap)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "2. Create bins for each full number of earthquake magnitude (for instance, the first bin\n",
        "is (0, 1], the second is (1, 2], and so on) with the ml magnitude type and count how\n",
        "many are in each bin.\n",
        "'''\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "pd.set_option('display.max_rows', 5000)\n",
        "pd.set_option('display.max_columns', 5000)\n",
        "\n",
        "earthquakes = pd.read_csv('https://raw.githubusercontent.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/master/ch_02/data/earthquakes.csv')\n",
        "\n",
        "ml = earthquakes[(earthquakes.magType == 'ml')]\n",
        "ml_max = ml.mag.max()\n",
        "#print(ml_max)   # Upper bound for the bins\n",
        "\n",
        "# This can also be done in one line: print(earthquakes[(earthquakes.magType == 'ml')].mag.max())\n",
        "\n",
        "earthquakes['bins'] = pd.cut(x=ml['mag'], bins = [0, 1, 2, 3, 4, 5, 6], labels = ['0 to 1','1 to 2', '2 to 3', '3 to 4', '4 to 5', '5 to 6'])\n",
        "counts = earthquakes['bins'].value_counts()\n",
        "counts.sort_index()\n"
      ],
      "metadata": {
        "id": "CwbSSOoNw_uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "3. Using the faang.csv file, group by the ticker and resample to monthly frequency.\n",
        "Make the following aggregations:\n",
        "a) Mean of the opening price\n",
        "b) Maximum of the high price\n",
        "c) Minimum of the low price\n",
        "d) Mean of the closing price\n",
        "e) Sum of the volume traded\n",
        "'''\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "pd.set_option('display.max_rows', 5000)\n",
        "pd.set_option('display.max_columns', 5000)\n",
        "\n",
        "META = pd.read_csv('https://raw.githubusercontent.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/master/ch_07/data/facebook.csv')\n",
        "AAPL = pd.read_csv('https://raw.githubusercontent.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/master/ch_07/data/apple.csv')\n",
        "AMZN = pd.read_csv('https://raw.githubusercontent.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/master/ch_07/data/amazon.csv')\n",
        "NFLX = pd.read_csv('https://raw.githubusercontent.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/master/ch_07/data/netflix.csv')\n",
        "GOOG = pd.read_csv('https://raw.githubusercontent.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/master/ch_07/data/google.csv')\n",
        "\n",
        "META['ticker'] = 'META'\n",
        "AAPL['ticker'] = 'AAPL'\n",
        "AMZN['ticker'] = 'AMZN'\n",
        "NFLX['ticker'] = 'NFLX'\n",
        "GOOG['ticker'] = 'GOOG'\n",
        "\n",
        "faang = META.append([AAPL, AMZN, NFLX, GOOG])\n",
        "\n",
        "faang['date'] = pd.to_datetime(faang['date']) \n",
        "faang = faang.set_index('date')\n",
        "\n",
        "# a) Mean of the opening price\n",
        "\n",
        "faang_copy = faang.copy()\n",
        "faang_copy.groupby('ticker')\n",
        "monthly_open = faang_copy.open.resample('M').mean()\n",
        "#display(monthly_open)\n",
        "\n",
        "# b) Maximum of the high price\n",
        "\n",
        "monthly_high = faang_copy.high.resample('M').max()\n",
        "#display(monthly_high)\n",
        "\n",
        "# c) Minimum of the low price\n",
        "\n",
        "monthly_low = faang_copy.low.resample('M').min()\n",
        "#display(monthly_low)\n",
        "\n",
        "# d) Mean of the closing price\n",
        "monthly_close = faang_copy.close.resample('M').mean()\n",
        "#display(monthly_close)\n",
        "\n",
        "# e) Sum of the volume traded\n",
        "\n",
        "monthly_volume = faang_copy.volume.resample('M').sum()\n",
        "#display(monthly_volume)\n",
        "\n"
      ],
      "metadata": {
        "id": "VILppvDihGy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "4. Build a crosstab with the earthquake data between the tsunami column and the\n",
        "magType column. Rather than showing the frequency count, show the maximum\n",
        "magnitude that was observed for each combination. Put the magnitude type along\n",
        "the columns.\n",
        "'''\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "\n",
        "earthquakes = pd.read_csv('https://raw.githubusercontent.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/master/ch_02/data/earthquakes.csv')\n",
        "\n",
        "df = earthquakes[['magType', 'tsunami', 'mag']]\n",
        "pd.crosstab(df.tsunami, df.magType, values = df.mag, aggfunc = np.max)"
      ],
      "metadata": {
        "id": "6kMoUfmGk1WS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "5. Calculate the rolling 60-day aggregations of the OHLC data by ticker for the\n",
        "FAANG data. Use the same aggregations as exercise 3.\n",
        "'''\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "pd.set_option('display.max_rows', 5000)\n",
        "pd.set_option('display.max_columns', 5000)\n",
        "\n",
        "ohlc = faang[['open', 'high', 'low', 'close', 'volume', 'ticker']].copy()\n",
        "ohlc.groupby('ticker')\n",
        "\n",
        "bimonthly_open = ohlc.open.rolling(60, min_periods = 1).mean()\n",
        "#display(bimonthly_open)\n",
        "\n",
        "bimonthly_high = ohlc.high.rolling(60, min_periods = 1).max()\n",
        "#display(bimonthly_high)\n",
        "\n",
        "bimonthly_low = ohlc.low.rolling(60, min_periods = 1).min()\n",
        "#display(bimonthly_low)\n",
        "\n",
        "bimonthly_close = ohlc.close.rolling(60, min_periods = 1).mean()\n",
        "#display(bimonthly_close)\n",
        "\n",
        "bimonthly_volume = ohlc.volume.rolling(60, min_periods = 1).sum()\n",
        "#display(bimonthly_volume)"
      ],
      "metadata": {
        "id": "Mn3AiPJrnfC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "6. Create a pivot table of the FAANG data that compares the stocks. Put the ticker in\n",
        "the rows and show the averages of the OHLC and volume traded data.\n",
        "\n",
        "'''\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "pd.set_option('display.max_rows', 10000)\n",
        "pd.set_option('display.max_columns', 10000)\n",
        "\n",
        "faang['date'] = pd.to_datetime(faang['date'])\n",
        "\n",
        "faang.pivot_table(index = 'ticker')\n"
      ],
      "metadata": {
        "id": "0PEVds37s7NO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is not an exercise from the book, just the result of experimenting.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "pd.set_option('display.max_rows', 10000)\n",
        "pd.set_option('display.max_columns', 10000)\n",
        "\n",
        "\n",
        "faang = pd.read_csv('/content/drive/MyDrive/Colab Datasets/faang.csv')\n",
        "faang['date'] = pd.to_datetime(faang['date'])\n",
        "gkk = faang.groupby(['date', 'ticker'])\n",
        "display(gkk.first(200))"
      ],
      "metadata": {
        "id": "jYnByALP2e3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Calculate the Z-scores for each numeric column of Amazon's data (ticker is\n",
        "# AMZN) in Q4 2018 using apply().\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import time\n",
        "\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.max_columns', 100)\n",
        "\n",
        "faang['date'] = pd.to_datetime(faang['date'])\n",
        "\n",
        "numeric = ['high', 'low', 'open', 'close', 'volume', 'adj_close']\n",
        "\n",
        "amzn = faang[(faang['ticker'] == 'AMZN') & (faang.date < '2020-01-01') & (faang.date.dt.quarter.isin([4]))]\n",
        "amzn = amzn.set_index(['date'])\n",
        "amzn = amzn[numeric]\n",
        "\n",
        "amzn.apply(lambda x : stats.zscore(x))"
      ],
      "metadata": {
        "id": "6NPgZhsRy-qX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "8. Add event descriptions:\n",
        "a) Create a dataframe with the following three columns: ticker, date, and\n",
        "event. The columns should have the following values:\n",
        "i) ticker: 'FB'\n",
        "ii) date: ['2018-07-25', '2018-03-19', '2018-03-20']\n",
        "iii) event: ['Disappointing user growth announced\n",
        "after close.', 'Cambridge Analytica story',\n",
        "'FTC investigation']\n",
        "b) Set the index to ['date', 'ticker'].\n",
        "c) Merge this data with the FAANG data using an outer join.\n",
        "'''\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.max_columns', 100)\n",
        "\n",
        "faang['date'] = pd.to_datetime(faang['date'])\n",
        "#faang = faang.set_index('date')\n",
        "faang2 = pd.DataFrame(\n",
        "    {'ticker':'META', \n",
        "     'date': ['2018-07-25', '2018-03-19', '2018-03-20'], \n",
        "     'event': ['Disappointing user growth announced after close.', 'Cambridge Analytica story', 'FTC investigation'] })\n",
        "faang2['date'] = pd.to_datetime(faang2['date'])\n",
        "faang2.set_index(['date', 'ticker'])\n",
        "faang3 = pd.merge(faang, faang2, on = 'date', how = 'outer')\n",
        "faang3\n"
      ],
      "metadata": {
        "id": "6EnmyX5M3uTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "9. Use the transform() method on the FAANG data to represent all the\n",
        "values in terms of the first date in the data. To do so, divide all the values for\n",
        "each ticker by the values for the first date in the data for that ticker. This is\n",
        "referred to as an index, and the data for the first date is the base (https://\n",
        "ec.europa.eu/eurostat/statistics-explained/index.php/\n",
        "Beginners:Statistical_concept_-_Index_and_base_year). When\n",
        "data is in this format, we can easily see growth over time. Hint: transform() can\n",
        "take a function name.\n",
        "'''\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.max_columns', 100)\n",
        "\n",
        "\n",
        "meta = faang[faang.ticker == 'META'].copy()\n",
        "aapl = faang[faang.ticker == 'AAPL'].copy()\n",
        "amzn = faang[faang.ticker == 'AMZN'].copy()\n",
        "nflx = faang[faang.ticker == 'NFLX'].copy()\n",
        "goog = faang[faang.ticker == 'GOOG'].copy()\n",
        "\n",
        "numeric = np.array(['high', 'low', 'open', 'close', 'volume', 'adj_close'])\n",
        "\n",
        "for column in numeric:\n",
        " meta[column] = np.array(meta[column]) / np.array(meta[column])[0]\n",
        " aapl[column] = np.array(aapl[column]) / np.array(aapl[column])[0]\n",
        " amzn[column] = np.array(amzn[column]) / np.array(amzn[column])[0]\n",
        " nflx[column] = np.array(nflx[column]) / np.array(nflx[column])[0]\n",
        " goog[column] = np.array(goog[column]) / np.array(goog[column])[0]\n",
        "\n",
        "faang1 = pd.concat([meta, aapl, amzn, nflx, goog])\n",
        "display(faang1)"
      ],
      "metadata": {
        "id": "7l5HmM1oU-IG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}